{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce5ffaf",
   "metadata": {},
   "source": [
    "# Task 4: Model Comparison & Selection\n",
    "\n",
    "In this task, we compare multiple NER models (e.g., XLM-Roberta, DistilBERT, mBERT) on Amharic entity extraction. We evaluate each model's accuracy, speed, and robustness, and select the best-performing model for production use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93473fa3",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Fine-tune multiple NER models (XLM-Roberta, DistilBERT, mBERT, etc.)\n",
    "2. Evaluate each model on the validation set\n",
    "3. Compare models based on accuracy, speed, and robustness\n",
    "4. Select the best-performing model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# List of models to compare\n",
    "model_names = [\n",
    "    'xlm-roberta-base',\n",
    "    'distilbert-base-multilingual-cased',\n",
    "    'bert-base-multilingual-cased',\n",
    "    # Add more models as needed\n",
    "]\n",
    "\n",
    "def load_model_and_tokenizer(model_name, num_labels, id2label, label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "# Assume train_dataset, val_dataset, label_list, id2label, label2id are already defined from previous steps\n",
    "results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    tokenizer, model = load_model_and_tokenizer(model_name, len(label_list), id2label, label2id)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{model_name}',\n",
    "        evaluation_strategy='epoch',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        logging_dir=f'./logs_{model_name}',\n",
    "        logging_steps=10,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda p: {'f1': f1_score([[id2label[l] for l in label if l != -100] for label in p.label_ids],\n",
    "                                                 [[id2label[pred] for (pred, l) in zip(prediction, label) if l != -100]\n",
    "                                                  for prediction, label in zip(np.argmax(p.predictions, axis=2), p.label_ids)])}\n",
    "    )\n",
    "    start = time.time()\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    elapsed = time.time() - start\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'f1': eval_result['eval_f1'],\n",
    "        'eval_time_sec': elapsed\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cfc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='f1', ascending=False)\n",
    "display(results_df)\n",
    "\n",
    "best_model = results_df.iloc[0]['model']\n",
    "print(f\"Best model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8c40b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Fine-tuned and evaluated multiple NER models (XLM-Roberta, DistilBERT, mBERT, etc.)\n",
    "- Compared models based on F1 score and evaluation time\n",
    "- Selected the best-performing model for Amharic entity extraction"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
