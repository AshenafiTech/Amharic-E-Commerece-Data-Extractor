{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93ecb04",
   "metadata": {},
   "source": [
    "# Task 3: Fine-Tune NER Model\n",
    "\n",
    "In this task, we fine-tune a Named Entity Recognition (NER) model to extract products, prices, and locations from Amharic Telegram messages. We use a pre-trained multilingual model (e.g., XLM-Roberta, bert-tiny-amharic, or afroxmlr) and Hugging Face's Trainer API for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226d89e",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Set up environment and install required libraries\n",
    "2. Load and parse the labeled CoNLL dataset\n",
    "3. Tokenize data and align labels\n",
    "4. Configure model and training arguments\n",
    "5. Fine-tune using Hugging Face Trainer\n",
    "6. Evaluate and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run in Colab or a GPU environment)\n",
    "!pip install transformers datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f5005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875aa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse the labeled CoNLL dataset\n",
    "\n",
    "def parse_conll(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens = []\n",
    "                    tags = []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                tokens.append(splits[0])\n",
    "                tags.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "sentences, ner_tags = parse_conll('../data/conll/labeled_subset.conll')\n",
    "print(sentences[0], ner_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5533c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for Hugging Face Trainer\n",
    "MODEL_NAME = 'xlm-roberta-base'  # or 'Davlan/bert-tiny-amharic', 'Davlan/afro-xlmr-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "label_list = sorted(set(l for tags in ner_tags for l in tags))\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "encodings = tokenizer(sentences, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "labels = []\n",
    "for i, label in enumerate(ner_tags):\n",
    "    word_ids = encodings.word_ids(batch_index=i)\n",
    "    label_ids = []\n",
    "    prev_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != prev_word_idx:\n",
    "            label_ids.append(label2id[label[word_idx]])\n",
    "        else:\n",
    "            label_ids.append(label2id[label[word_idx]] if label[word_idx].startswith('I-') else -100)\n",
    "        prev_word_idx = word_idx\n",
    "    labels.append(label_ids)\n",
    "encodings['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face Dataset object\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': encodings['input_ids'],\n",
    "    'attention_mask': encodings['attention_mask'],\n",
    "    'labels': encodings['labels']\n",
    "})\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74690f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model and training arguments\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b0a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        'f1': f1_score(true_labels, true_predictions),\n",
    "        'report': classification_report(true_labels, true_predictions)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results['eval_report'])\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./fine_tuned_ner_model')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
