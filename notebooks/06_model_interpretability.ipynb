{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c335d4",
   "metadata": {},
   "source": [
    "# Task 5: Model Interpretability\n",
    "\n",
    "In this task, we use interpretability tools such as SHAP and LIME to explain how the NER model identifies entities in Amharic Telegram messages. This helps ensure transparency, trust, and guides further model improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14f40f",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Install and import SHAP and LIME\n",
    "2. Load the fine-tuned NER model\n",
    "3. Use SHAP and LIME to interpret model predictions\n",
    "4. Analyze difficult/ambiguous cases\n",
    "5. Generate interpretability reports and identify areas for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b664c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP and LIME\n",
    "!pip install shap lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = '../models/fine_tuned_ner_model'  # Update path as needed\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prediction function for LIME/SHAP\n",
    "\n",
    "def predict_proba(texts):\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    # For demonstration, return softmax over the first token (can be adapted for sequence labeling)\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=-1)\n",
    "    return probs[:, 0, :].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME explanation for a sample message\n",
    "explainer = LimeTextExplainer(class_names=list(model.config.id2label.values()))\n",
    "sample_text = \"አዲስ ምርት ዋጋ 1000 ብር ቦሌ ሱቅ\"\n",
    "exp = explainer.explain_instance(sample_text, predict_proba, num_features=10)\n",
    "exp.show_in_notebook(text=sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f41f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for a sample message\n",
    "explainer = shap.Explainer(predict_proba, tokenizer)\n",
    "shap_values = explainer([sample_text])\n",
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0313c980",
   "metadata": {},
   "source": [
    "## Analysis of Difficult Cases and Summary\n",
    "\n",
    "- Review LIME and SHAP explanations for ambiguous or overlapping entities.\n",
    "- Identify patterns where the model struggles (e.g., similar product and location names, price ambiguity).\n",
    "- Use interpretability insights to suggest improvements (e.g., more training data, better tokenization, or model architecture adjustments).\n",
    "\n",
    "This interpretability analysis increases trust in the NER system and guides future enhancements."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
